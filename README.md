# L5-DE-Data-Pipelines-T5

This is the final part of the pipeline build! Today is adding one more set of data and "cleaning up" your code.

## ðŸš€ Project Overview

Your objectives now are listed below:
   - Add one more set of data.
   - Check your code for any issues relating to the latest set of data.
   - Make sure you code is clean, well documented, and 


**Note: Collaborate with Your Instructor**  
 Your instructor will act as the stakeholder and Subject Matter Expert (SME). They will provide clarification on:
   - Schema requirements
   - Business rules for validation
   - Future data sources

---

## ðŸ”„ Data Pipeline Enhancements
The pipeline should been updated to process UK, FR, US, and SC datasets:
- Config files allow defining of parameters spcific to each file type.
- Clean and normalise data fields (e.g., DoB formats, salary parsing, gender codes).
- Add the appropriate 'country_code' (e.g., 'UK', 'FR') for each user entry.
- Merge data into a shared 'users' and 'logins' table with consistent structure.

---

## ðŸ“¦ Repository Structure

```plaintext
â”œâ”€â”€ config/                         # JSON configs for mappings and exclusions
â”‚   â”œâ”€â”€ exclusions.json
â”‚   â”œâ”€â”€ mappings_uk.json
â”‚   â”œâ”€â”€ mappings_fr.json
â”‚   â”œâ”€â”€ mappings_usa.json
|   â””â”€â”€ mappings_sc.json
â”‚
â”œâ”€â”€ data/                           # Raw CSV data files
â”‚   â”œâ”€â”€ FR User Data.csv
â”‚   â”œâ”€â”€ FR-User-LoginTS.csv
â”‚   â”œâ”€â”€ SC User Data.csv
â”‚   â”œâ”€â”€ SC-User-LoginTS.csv
â”‚   â”œâ”€â”€ UK User Data.csv
â”‚   â”œâ”€â”€ UK-User-LoginTS.csv
â”‚   â”œâ”€â”€ USA User Data.csv
â”‚   â””â”€â”€ USA-User-LoginTS.csv
â”‚
â”œâ”€â”€ pipeline/                       # Python package for reusable pipeline code
â”‚   â”œâ”€â”€ __init__.py                 # Empty file to make this a package
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ data_utils.py               # utility functions (mostly for transforming data)
â”‚   â”œâ”€â”€ database.py                 # Python code related to database management
â”‚   â””â”€â”€ transform.py                # workhorse of transformations
â”‚
â”œâ”€â”€ sql/                            # SQL schema script
â”‚   â””â”€â”€ create_database.sql
â”‚
â”œâ”€â”€ tests/                          # Test coverage
â”‚   â”œâ”€â”€ test_pipeline.py            # testing for issues with code
â”‚   â””â”€â”€ validation.py               # validating data
â”‚
â”œâ”€â”€ data_pipeline.py                # Starting point for your script
â”œâ”€â”€ main.py                         # Main orchestration script
â”œâ”€â”€ requirements.txt                # Optional dependency list
â”œâ”€â”€ README.md                       # Project documentation
```

You will work from `data_pipeline.py`, and create the modularised content in `config`, `data`, `pipeline`, `sql`, `tests` and `main.py`.

---

## ðŸ§  Remember

This setup uses **SQLite** for simplicity and local prototyping, but the same schema and logic should be portable to PostgreSQL or another RDBMS in a production environment. Keep modularity and maintainability in mind.

Happy coding!